# LoRA Fine-Tuning on LLaMA 2 - 7B Parameter Model 

## Overview
This repository demonstrates Low-Rank Adaptation (LoRA) fine-tuning on the LLaMA 2 - 7 billion parameter model. By using LoRA, only 0.06% of the model's parameters are trained, making it significantly more efficient than full fine-tuning. The model is trained for a basic multiclass classification task: predicting the category of a product based on its name, using the Walmart dataset

